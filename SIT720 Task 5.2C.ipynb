{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c005c79",
   "metadata": {},
   "source": [
    "# SIT720\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "# TASK 5.2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed16754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the DataFrame:\n",
      "   ID  N_Days Status             Drug    Age Sex Ascites Hepatomegaly Spiders  \\\n",
      "0   1     400      D  D-penicillamine  21464   F       Y            Y       Y   \n",
      "1   2    4500      C  D-penicillamine  20617   F       N            Y       Y   \n",
      "2   3    1012      D  D-penicillamine  25594   M       N            N       N   \n",
      "3   4    1925      D  D-penicillamine  19994   F       N            Y       Y   \n",
      "4   5    1504     CL          Placebo  13918   F       N            Y       Y   \n",
      "\n",
      "  Edema  Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  \\\n",
      "0     Y       14.5        261.0     2.60   156.0    1718.0  137.95   \n",
      "1     N        1.1        302.0     4.14    54.0    7394.8  113.52   \n",
      "2     S        1.4        176.0     3.48   210.0     516.0   96.10   \n",
      "3     S        1.8        244.0     2.54    64.0    6121.8   60.63   \n",
      "4     N        3.4        279.0     3.53   143.0     671.0  113.15   \n",
      "\n",
      "   Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0          172.0      190.0         12.2      4  \n",
      "1           88.0      221.0         10.6      3  \n",
      "2           55.0      151.0         12.0      4  \n",
      "3           92.0      183.0         10.3      4  \n",
      "4           72.0      136.0         10.9      3  \n",
      "Null values in each column:\n",
      "ID                0\n",
      "N_Days            0\n",
      "Status            0\n",
      "Drug              0\n",
      "Age               0\n",
      "Sex               0\n",
      "Ascites           0\n",
      "Hepatomegaly      0\n",
      "Spiders           0\n",
      "Edema             0\n",
      "Bilirubin         0\n",
      "Cholesterol      28\n",
      "Albumin           0\n",
      "Copper            2\n",
      "Alk_Phos          0\n",
      "SGOT              0\n",
      "Tryglicerides    30\n",
      "Platelets         4\n",
      "Prothrombin       0\n",
      "Stage             0\n",
      "dtype: int64\n",
      "\n",
      "Size of the dataset after dropping rows with missing categorical values: (312, 20)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"cis.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(data.head())\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "print(\"Null values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Assuming 'data' is your original DataFrame with missing values\n",
    "# You can fill missing values with a specified value, such as 0\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Calculate and display the size of the dataset\n",
    "size = data_filled.shape\n",
    "print(\"\\nSize of the dataset after dropping rows with missing categorical values:\", size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd69694",
   "metadata": {},
   "source": [
    "Reading CSV file: It imports the pandas library and reads a CSV file named \"cirrhosis.csv\" using the pd.read_csv() function. The data read from the CSV file is stored in a DataFrame named data.\n",
    "\n",
    "Displaying the first few rows: It prints the first few rows of the DataFrame using the head() method, which provides a quick view of the data structure and contents.\n",
    "\n",
    "Calculating null values: It calculates the number of missing values in each column of the DataFrame using the isnull() method to create a boolean DataFrame where missing values are marked as True, and then applies the sum() method to count the number of True values for each column.\n",
    "\n",
    "Printing null values: It prints the number of null values in each column using the print() function.\n",
    "\n",
    "Calculating dataset size: It calculates the size of the DataFrame after dropping rows with missing categorical values. However, the code snippet doesn't actually drop any rows or fill missing values, so the data_filled DataFrame referenced here seems to be missing in the provided code. Hence, this part of the code may raise an error. It calculates the size using the shape attribute, which returns a tuple containing the number of rows and columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c62afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled dataset saved to cirrhosis_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the filled dataset to a new CSV file\n",
    "data_filled.to_csv(\"cirrhosis_test.csv\", index=False)\n",
    "\n",
    "print(\"Filled dataset saved to cirrhosis_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adeacb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Information:\n",
      "Number of samples in training data: 224\n",
      "Number of features in training data: 17\n",
      "\n",
      "Missing values in Training Data:\n",
      "Drug             0\n",
      "Age              0\n",
      "Sex              0\n",
      "Ascites          0\n",
      "Hepatomegaly     0\n",
      "Spiders          0\n",
      "Edema            0\n",
      "Bilirubin        0\n",
      "Cholesterol      0\n",
      "Albumin          0\n",
      "Copper           0\n",
      "Alk_Phos         0\n",
      "SGOT             0\n",
      "Tryglicerides    0\n",
      "Platelets        0\n",
      "Prothrombin      0\n",
      "Stage            0\n",
      "dtype: int64\n",
      "\n",
      "Test Data Information:\n",
      "Number of samples in test data: 88\n",
      "Number of features in test data: 17\n",
      "\n",
      "Missing values in Test Data:\n",
      "Drug             0\n",
      "Age              0\n",
      "Sex              0\n",
      "Ascites          0\n",
      "Hepatomegaly     0\n",
      "Spiders          0\n",
      "Edema            0\n",
      "Bilirubin        0\n",
      "Cholesterol      0\n",
      "Albumin          0\n",
      "Copper           0\n",
      "Alk_Phos         0\n",
      "SGOT             0\n",
      "Tryglicerides    0\n",
      "Platelets        0\n",
      "Prothrombin      0\n",
      "Stage            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extracting features and labels for training and testing\n",
    "X_train = data_filled.iloc[:224, 3:]  \n",
    "y_train = data_filled.iloc[:224, 2]   \n",
    "X_test = data_filled.iloc[224:, 3:]   \n",
    "y_test = data_filled.iloc[224:, 2]    \n",
    "\n",
    "# Display training data information\n",
    "print(\"Training Data Information:\")\n",
    "print(\"Number of samples in training data:\", len(X_train))\n",
    "print(\"Number of features in training data:\", len(X_train.columns))\n",
    "\n",
    "# Display missing values in training data\n",
    "print(\"\\nMissing values in Training Data:\")\n",
    "print(X_train.isnull().sum())\n",
    "\n",
    "# Display test data information\n",
    "print(\"\\nTest Data Information:\")\n",
    "print(\"Number of samples in test data:\", len(X_test))\n",
    "print(\"Number of features in test data:\", len(X_test.columns))\n",
    "\n",
    "# Display missing values in test data\n",
    "print(\"\\nMissing values in Test Data:\")\n",
    "print(X_test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c26fbc",
   "metadata": {},
   "source": [
    "Extracting features and labels: It extracts features and labels for training and testing from the DataFrame data_filled. For training data, it selects the first 224 rows and all columns starting from the 4th column (index 3) to the last column. For testing data, it selects rows from index 224 to the end for both features and labels.\n",
    "\n",
    "Displaying training data information: It prints information about the training data, including the number of samples (rows) and the number of features (columns) using the len() function and the shape attribute.\n",
    "\n",
    "Displaying missing values in training data: It prints the number of missing values for each feature in the training data using the isnull() method followed by the sum() method to count the missing values for each column.\n",
    "\n",
    "Displaying test data information: Similar to training data, it prints information about the test data, including the number of samples (rows) and the number of features (columns) using the len() function and the shape attribute.\n",
    "\n",
    "Displaying missing values in test data: It prints the number of missing values for each feature in the test data using the isnull() method followed by the sum() method to count the missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af4415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Training Data after imputation:\n",
      "num__Age                     0\n",
      "num__Bilirubin               0\n",
      "num__Cholesterol             0\n",
      "num__Albumin                 0\n",
      "num__Copper                  0\n",
      "num__Alk_Phos                0\n",
      "num__SGOT                    0\n",
      "num__Tryglicerides           0\n",
      "num__Platelets               0\n",
      "num__Prothrombin             0\n",
      "num__Stage                   0\n",
      "cat__Drug_D-penicillamine    0\n",
      "cat__Drug_Placebo            0\n",
      "cat__Sex_F                   0\n",
      "cat__Sex_M                   0\n",
      "cat__Ascites_N               0\n",
      "cat__Ascites_Y               0\n",
      "cat__Hepatomegaly_N          0\n",
      "cat__Hepatomegaly_Y          0\n",
      "cat__Spiders_N               0\n",
      "cat__Spiders_Y               0\n",
      "cat__Edema_N                 0\n",
      "cat__Edema_S                 0\n",
      "cat__Edema_Y                 0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Test Data after imputation:\n",
      "num__Age                     0\n",
      "num__Bilirubin               0\n",
      "num__Cholesterol             0\n",
      "num__Albumin                 0\n",
      "num__Copper                  0\n",
      "num__Alk_Phos                0\n",
      "num__SGOT                    0\n",
      "num__Tryglicerides           0\n",
      "num__Platelets               0\n",
      "num__Prothrombin             0\n",
      "num__Stage                   0\n",
      "cat__Drug_D-penicillamine    0\n",
      "cat__Drug_Placebo            0\n",
      "cat__Sex_F                   0\n",
      "cat__Sex_M                   0\n",
      "cat__Ascites_N               0\n",
      "cat__Ascites_Y               0\n",
      "cat__Hepatomegaly_N          0\n",
      "cat__Hepatomegaly_Y          0\n",
      "cat__Spiders_N               0\n",
      "cat__Spiders_Y               0\n",
      "cat__Edema_N                 0\n",
      "cat__Edema_S                 0\n",
      "cat__Edema_Y                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Find categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values for numerical features with mean\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ~X_train.columns.isin(categorical_cols)),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Preprocess training data\n",
    "X_train_imputed = pd.DataFrame(preprocessor.fit_transform(X_train), columns=preprocessor.get_feature_names_out(input_features=X_train.columns))\n",
    "\n",
    "# Preprocess test data\n",
    "X_test_imputed = pd.DataFrame(preprocessor.transform(X_test), columns=preprocessor.get_feature_names_out(input_features=X_test.columns))\n",
    "\n",
    "# Display missing values after imputation\n",
    "print(\"Missing values in Training Data after imputation:\")\n",
    "print(X_train_imputed.isnull().sum())\n",
    "print(\"\\nMissing values in Test Data after imputation:\")\n",
    "print(X_test_imputed.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2b3c2",
   "metadata": {},
   "source": [
    "Finding categorical columns: It identifies the categorical columns in the training data (X_train) using the select_dtypes method with the include=['object'] parameter, which selects columns of object data type.\n",
    "\n",
    "Imputing missing values for numerical features: It initializes a SimpleImputer object to impute missing values for numerical features with the mean value. This imputer will replace missing values in numerical columns with the mean of the respective columns.\n",
    "\n",
    "One-hot encoding categorical features: It initializes a OneHotEncoder object to perform one-hot encoding on categorical features. One-hot encoding converts categorical variables into binary vectors, which can be used as features in machine learning models.\n",
    "\n",
    "Combining transformers: It creates a ColumnTransformer object named preprocessor to combine the imputation and one-hot encoding transformers. The ColumnTransformer applies different transformations to different columns of the input data.\n",
    "\n",
    "Preprocessing training and test data: It preprocesses the training and test data using the fit_transform method of the ColumnTransformer for the training data and the transform method for the test data. This applies the imputation and one-hot encoding transformations to the respective datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e6f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution based on the Training Data:\n",
      "Status\n",
      "D     114\n",
      "C     103\n",
      "CL      7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "The training set is not balanced.\n"
     ]
    }
   ],
   "source": [
    "# Calculate label distribution\n",
    "label_distribution = y_train.value_counts()\n",
    "\n",
    "# Display label distribution\n",
    "print(\"Label Distribution based on the Training Data:\")\n",
    "print(label_distribution)\n",
    "\n",
    "# Check if the dataset is balanced\n",
    "balance_threshold = 0.5\n",
    "min_count = label_distribution.min()\n",
    "max_count = label_distribution.max()\n",
    "is_balanced = min_count / max_count >= balance_threshold\n",
    "\n",
    "# Print result\n",
    "if is_balanced:\n",
    "    print(\"\\nThe training set is balanced.\")\n",
    "else:\n",
    "    print(\"\\nThe training set is not balanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bc327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution based on the Test Data:\n",
      "Status\n",
      "C     65\n",
      "CL    12\n",
      "D     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "The test set is not balanced.\n"
     ]
    }
   ],
   "source": [
    "# Calculate label distribution for the test data\n",
    "test_label_distribution = y_test.value_counts()\n",
    "\n",
    "# Display label distribution for the test data\n",
    "print(\"Label Distribution based on the Test Data:\")\n",
    "print(test_label_distribution)\n",
    "\n",
    "# Check if the test dataset is balanced\n",
    "test_min_count = test_label_distribution.min()\n",
    "test_max_count = test_label_distribution.max()\n",
    "test_is_balanced = test_min_count / test_max_count >= balance_threshold\n",
    "\n",
    "# Print result for the test data\n",
    "if test_is_balanced:\n",
    "    print(\"\\nThe test set is balanced.\")\n",
    "else:\n",
    "    print(\"\\nThe test set is not balanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61f9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\n",
      "Accuracy: 0.6136\n",
      "Precision: 0.7492\n",
      "Recall: 0.6136\n",
      "F1 Score: 0.6364\n",
      "Random Forest Classifier:\n",
      "Accuracy: 0.6023\n",
      "Precision: 0.8884\n",
      "Recall: 0.6023\n",
      "F1 Score: 0.6236\n",
      "SVM Classifier:\n",
      "Accuracy: 0.6364\n",
      "Precision: 0.8373\n",
      "Recall: 0.6364\n",
      "F1 Score: 0.6342\n",
      "Logistic Regression Classifier:\n",
      "Accuracy: 0.6591\n",
      "Precision: 0.8422\n",
      "Recall: 0.6591\n",
      "F1 Score: 0.6519\n",
      "KNN Classifier:\n",
      "Accuracy: 0.6136\n",
      "Precision: 0.8457\n",
      "Recall: 0.6136\n",
      "F1 Score: 0.6201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Create instances of the classifiers\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    clf = make_pipeline(StandardScaler(), model)\n",
    "    clf.fit(X_train_imputed, y_train)\n",
    "    y_pred = clf.predict(X_test_imputed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"{name} Classifier:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ddf9a",
   "metadata": {},
   "source": [
    "The dataset is first preprocessed to handle missing values and encode categorical variables. The following classifiers are used: Decision Tree, Random Forest, Support Vector Machine (SVM), Logistic Regression, and k-Nearest Neighbors (KNN).\n",
    "\n",
    "Preprocessing:\n",
    "Missing values in X_train and X_test are imputed using a column transformer that applies the mean strategy for numerical features and one-hot encoding for categorical features. This results in X_train_imputed and X_test_imputed.\n",
    "\n",
    "Model Training and Evaluation:\n",
    "Each model is instantiated and stored in a dictionary called models.\n",
    "For each model, a pipeline is created that includes StandardScaler for feature scaling followed by the classifier.\n",
    "The model is trained (fit) on the training data (X_train_imputed and y_train).\n",
    "Predictions (y_pred) are made on the test data (X_test_imputed).\n",
    "\n",
    "Metrics Calculation:\n",
    "Accuracy, precision, recall, and F1 score are calculated for each model using the true labels (y_test) and the predictions (y_pred).\n",
    "These metrics evaluate the model's performance, with the following interpretations:\n",
    "Accuracy: The proportion of correctly predicted instances.\n",
    "Precision: The proportion of true positive predictions among all positive predictions.\n",
    "Recall: The proportion of true positive predictions among all actual positives.\n",
    "F1 Score: The harmonic mean of precision and recall, balancing both.\n",
    "\n",
    "Output:\n",
    "The results for each classifier are printed, showing their performance on the test set. For example, Logistic Regression achieves the highest accuracy of 65.91%, with an F1 score of 65.19%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da060a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define and train the Logistic Regression model\n",
    "logistic_model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "logistic_model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predict on the test data using the trained Logistic Regression model\n",
    "predictions_lr = logistic_model.predict(X_test_imputed)\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "predictions_lr_df = pd.DataFrame({'testID': np.arange(1, len(predictions_lr) + 1), 'Status': predictions_lr})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_lr_df.to_csv(\"predictions-logistic-regression-test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37e1b4",
   "metadata": {},
   "source": [
    "Model Training and Evaluation:\n",
    "\n",
    "Various classifiers were trained and evaluated, including Decision Tree, Random Forest, SVM, Logistic Regression, and KNN.\n",
    "Logistic Regression achieved the highest accuracy (65.91%) among the evaluated models, indicating it was the best-performing model on this dataset.\n",
    "\n",
    "Final Model:\n",
    "Logistic Regression was selected for further predictions. The model was trained on the preprocessed training data using a pipeline that included feature scaling.\n",
    "Predictions were made on the test data and saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6e9cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation Feature Importance for Logistic Regression:\n",
      "                      Feature  Importance\n",
      "1              num__Bilirubin    0.064773\n",
      "0                    num__Age    0.039773\n",
      "4                 num__Copper    0.028409\n",
      "16             cat__Ascites_Y    0.020455\n",
      "15             cat__Ascites_N    0.020455\n",
      "9            num__Prothrombin    0.019318\n",
      "18        cat__Hepatomegaly_Y    0.012500\n",
      "17        cat__Hepatomegaly_N    0.012500\n",
      "23               cat__Edema_Y    0.012500\n",
      "10                 num__Stage    0.007955\n",
      "2            num__Cholesterol    0.004545\n",
      "7          num__Tryglicerides    0.002273\n",
      "3                num__Albumin    0.002273\n",
      "8              num__Platelets    0.001136\n",
      "13                 cat__Sex_F    0.001136\n",
      "14                 cat__Sex_M    0.001136\n",
      "11  cat__Drug_D-penicillamine    0.000000\n",
      "5               num__Alk_Phos    0.000000\n",
      "21               cat__Edema_N    0.000000\n",
      "12          cat__Drug_Placebo    0.000000\n",
      "6                   num__SGOT   -0.001136\n",
      "19             cat__Spiders_N   -0.004545\n",
      "20             cat__Spiders_Y   -0.004545\n",
      "22               cat__Edema_S   -0.005682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance for the Logistic Regression model\n",
    "result_lr = permutation_importance(logistic_model, X_test_imputed, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importance_perm_lr = pd.DataFrame({'Feature': X_test_imputed.columns, 'Importance': result_lr.importances_mean})\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "feature_importance_perm_lr = feature_importance_perm_lr.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print permutation feature importance for Logistic Regression\n",
    "print(\"Permutation Feature Importance for Logistic Regression:\")\n",
    "print(feature_importance_perm_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0683cd",
   "metadata": {},
   "source": [
    "key Predictors: The most important features for predicting the target variable are bilirubin, age, and copper levels.\n",
    "\n",
    "Less Relevant Features: Features like SGOT, presence of spiders, and certain drug types have little to no positive impact on model performance.\n",
    "\n",
    "Model Refinement: Based on these results, further model refinement could involve focusing on the high and moderately important features and possibly excluding or re-evaluating the less relevant ones to enhance model efficiency and performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
