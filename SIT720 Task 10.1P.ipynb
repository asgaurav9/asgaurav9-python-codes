{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5371c5",
   "metadata": {},
   "source": [
    "# SIT720\n",
    "Machine Learning\n",
    "\n",
    "Task 10.1P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c230e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\asgau\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asgau\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f4c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asgau\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1171 - val_accuracy: 0.9852 - val_loss: 0.0675\n",
      "Epoch 2/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9842 - loss: 0.0552 - val_accuracy: 0.9877 - val_loss: 0.0508\n",
      "Epoch 3/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9876 - loss: 0.0392 - val_accuracy: 0.9868 - val_loss: 0.0756\n",
      "Epoch 4/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9904 - loss: 0.0293 - val_accuracy: 0.9883 - val_loss: 0.0548\n",
      "Epoch 5/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9927 - loss: 0.0218 - val_accuracy: 0.9873 - val_loss: 0.0596\n",
      "Epoch 6/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9940 - loss: 0.0178 - val_accuracy: 0.9843 - val_loss: 0.0558\n",
      "Epoch 7/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9944 - loss: 0.0154 - val_accuracy: 0.9877 - val_loss: 0.0957\n",
      "Epoch 8/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0121 - val_accuracy: 0.9877 - val_loss: 0.0840\n",
      "Epoch 9/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0120 - val_accuracy: 0.9868 - val_loss: 0.1168\n",
      "Epoch 10/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0119 - val_accuracy: 0.9876 - val_loss: 0.0862\n",
      "Epoch 11/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9953 - loss: 0.0143 - val_accuracy: 0.9862 - val_loss: 0.1588\n",
      "Epoch 12/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0088 - val_accuracy: 0.9866 - val_loss: 0.0934\n",
      "Epoch 13/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9868 - val_loss: 0.1410\n",
      "Epoch 14/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0095 - val_accuracy: 0.9866 - val_loss: 0.1502\n",
      "Epoch 15/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0093 - val_accuracy: 0.9860 - val_loss: 0.0760\n",
      "Epoch 16/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0158 - val_accuracy: 0.9860 - val_loss: 0.1039\n",
      "Epoch 17/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9979 - loss: 0.0075 - val_accuracy: 0.9865 - val_loss: 0.0911\n",
      "Epoch 18/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0086 - val_accuracy: 0.9860 - val_loss: 0.0984\n",
      "Epoch 19/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0072 - val_accuracy: 0.9869 - val_loss: 0.0935\n",
      "Epoch 20/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9988 - loss: 0.0059 - val_accuracy: 0.9862 - val_loss: 0.0991\n",
      "Epoch 21/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0055 - val_accuracy: 0.9870 - val_loss: 0.0882\n",
      "Epoch 22/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9979 - loss: 0.0078 - val_accuracy: 0.9848 - val_loss: 0.1125\n",
      "Epoch 23/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9978 - loss: 0.0079 - val_accuracy: 0.9838 - val_loss: 0.0786\n",
      "Epoch 24/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 0.9879 - val_loss: 0.0951\n",
      "Epoch 25/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9977 - loss: 0.0071 - val_accuracy: 0.9870 - val_loss: 0.0833\n",
      "Epoch 26/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0054 - val_accuracy: 0.9880 - val_loss: 0.1647\n",
      "Epoch 27/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0046 - val_accuracy: 0.9866 - val_loss: 0.0953\n",
      "Epoch 28/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0046 - val_accuracy: 0.9886 - val_loss: 0.1646\n",
      "Epoch 29/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0112 - val_accuracy: 0.9866 - val_loss: 0.1451\n",
      "Epoch 30/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0068 - val_accuracy: 0.9872 - val_loss: 0.1940\n",
      "Epoch 31/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 0.9859 - val_loss: 0.1992\n",
      "Epoch 32/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0046 - val_accuracy: 0.9868 - val_loss: 0.2208\n",
      "Epoch 33/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0065 - val_accuracy: 0.9856 - val_loss: 0.0858\n",
      "Epoch 34/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0072 - val_accuracy: 0.9849 - val_loss: 0.1677\n",
      "Epoch 35/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0055 - val_accuracy: 0.9869 - val_loss: 0.2165\n",
      "Epoch 36/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 0.9866 - val_loss: 0.1178\n",
      "Epoch 37/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0043 - val_accuracy: 0.9889 - val_loss: 0.1867\n",
      "Epoch 38/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0058 - val_accuracy: 0.9875 - val_loss: 0.0891\n",
      "Epoch 39/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0058 - val_accuracy: 0.9875 - val_loss: 0.1078\n",
      "Epoch 40/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9976 - loss: 0.0107 - val_accuracy: 0.9879 - val_loss: 0.1122\n",
      "Epoch 41/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0049 - val_accuracy: 0.9873 - val_loss: 0.0610\n",
      "Epoch 42/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9968 - loss: 0.0112 - val_accuracy: 0.9868 - val_loss: 0.1151\n",
      "Epoch 43/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0047 - val_accuracy: 0.9886 - val_loss: 0.1525\n",
      "Epoch 44/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9989 - loss: 0.0033 - val_accuracy: 0.9860 - val_loss: 0.2443\n",
      "Epoch 45/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 0.9880 - val_loss: 0.1134\n",
      "Epoch 46/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0037 - val_accuracy: 0.9876 - val_loss: 0.1005\n",
      "Epoch 47/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 0.9860 - val_loss: 0.1585\n",
      "Epoch 48/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 0.9866 - val_loss: 0.1799\n",
      "Epoch 49/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 0.9866 - val_loss: 0.2149\n",
      "Epoch 50/50\n",
      "\u001b[1m878/878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9987 - loss: 0.0038 - val_accuracy: 0.9885 - val_loss: 0.3135\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step\n",
      "Accuracy: 0.9858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.51      0.65       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.94      0.76      0.82      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('data_10.csv')\n",
    "\n",
    "# Assuming the last column is the target \n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "\n",
    "for _ in range(10):\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "\n",
    "if len(np.unique(y)) == 2:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary \n",
    "if len(np.unique(y)) == 2:\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "else:\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "if len(np.unique(y)) == 2:\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(classification_report(y_test, y_pred, target_names=[str(i) for i in np.unique(y)]))\n",
    "\n",
    "# Additional metrics for regression\n",
    "if len(np.unique(y)) > 2:  \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asgau\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step\n",
      "Activation Function: relu\n",
      "Accuracy: 0.9855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.55      0.66       226\n",
      "           1       0.99      1.00      0.99      8550\n",
      "\n",
      "    accuracy                           0.99      8776\n",
      "   macro avg       0.91      0.78      0.83      8776\n",
      "weighted avg       0.98      0.99      0.98      8776\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asgau\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to build and train the MLP model\n",
    "def build_and_evaluate_model(activation_function):\n",
    "    model = Sequential()\n",
    "\n",
    "    \n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation=activation_function))\n",
    "\n",
    "    \n",
    "    for _ in range(10):\n",
    "        model.add(Dense(64, activation=activation_function))\n",
    "\n",
    "    if len(np.unique(y)) == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert predictions to binary\n",
    "    if len(np.unique(y)) == 2:\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "    else:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Activation Function: {activation_function}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    if len(np.unique(y)) == 2:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    else:\n",
    "        print(classification_report(y_test, y_pred, target_names=[str(i) for i in np.unique(y)]))\n",
    "\n",
    "    # Additional metrics for regression\n",
    "    if len(np.unique(y)) > 2:  \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f'Mean Squared Error: {mse:.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# List of activation functions to evaluate\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Evaluate the model\n",
    "for activation_function in activation_functions:\n",
    "    build_and_evaluate_model(activation_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559e190",
   "metadata": {},
   "source": [
    "The model was trained using the ReLU activation function for 50 epochs, and the performance metrics were recorded for each epoch. Here is a detailed analysis based on the training logs:\n",
    "\n",
    "Training Accuracy and Loss:\n",
    "\n",
    "The model started with a training accuracy of 97.8% and a training loss of 0.1152.\n",
    "By the end of 50 epochs, the training accuracy reached 99.92%, and the training loss reduced to 0.0042.\n",
    "The training process was stable, with continuous improvement in accuracy and reduction in loss over epochs.\n",
    "\n",
    "Validation Accuracy and Loss:\n",
    "\n",
    "The initial validation accuracy was 98.58%, with a validation loss of 0.0523.\n",
    "The validation accuracy slightly fluctuated but remained high, ending at 98.60% after 50 epochs.\n",
    "The validation loss showed more variation, increasing to 0.2181 by the end of the training period.\n",
    "The fluctuation in validation loss indicates potential overfitting as the training progressed.\n",
    "\n",
    "Classification Performance:\n",
    "\n",
    "The final test accuracy was 98.46%.\n",
    "The classification report provides precision, recall, and F1-score for each class:\n",
    "Class 0 (Minority class): Precision = 0.75, Recall = 0.60, F1-score = 0.67\n",
    "Class 1 (Majority class): Precision = 0.99, Recall = 0.99, F1-score = 0.99\n",
    "The macro average F1-score was 0.83, indicating that the model handled the imbalanced dataset reasonably well.\n",
    "The weighted average F1-score was 0.98, which is more representative given the imbalance in class distribution.\n",
    "\n",
    "Best Performance\n",
    "Activation Function: ReLU\n",
    "Final Test Accuracy: 98.46%\n",
    "Final Classification Metrics:\n",
    "Class 0 (Minority class): Precision = 0.76, Recall = 0.60, F1-score = 0.67\n",
    "Class 1 (Majority class): Precision = 0.99, Recall = 1.00, F1-score = 0.99\n",
    "Overall Accuracy: 98.48%\n",
    "Macro Average F1-score: 0.83\n",
    "Weighted Average F1-score: 0.98\n",
    "\n",
    "Conclusion\n",
    "Based on the performance metrics and training logs, the following conclusions can be drawn:\n",
    "\n",
    "High Accuracy and Precision: The model achieved very high accuracy and precision, especially for the majority class. This indicates the model's effectiveness in correctly identifying the dominant class in the dataset.\n",
    "\n",
    "Class Imbalance Handling: The model struggled somewhat with the minority class (Class 0), as evidenced by the lower recall and F1-score. This is a common challenge in imbalanced datasets.\n",
    "\n",
    "Overfitting: There are signs of overfitting, as indicated by the increasing validation loss despite the high accuracy. This could be mitigated by techniques such as regularization, dropout, or early stopping.\n",
    "\n",
    "ReLU Activation: The ReLU activation function proved to be very effective, providing excellent performance metrics and stability during training. ReLU is known for handling vanishing gradient problems well, which likely contributed to the model's success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
